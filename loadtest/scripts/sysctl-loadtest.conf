# =============================================================================
# Whisper Load Test Client - Kernel Tuning
# =============================================================================
#
# Apply with:  sudo sysctl -p loadtest/scripts/sysctl-loadtest.conf
# Or run:      sudo ./loadtest/scripts/tune-client.sh
#
# Optimized for generating 100K-1M outbound WebSocket connections from a
# single load test client machine.
#
# This is the CLIENT-SIDE companion to the server tuning in:
#   scripts/sysctl-whisper.conf   (server kernel params)
#   scripts/tune-kernel.sh        (server tuning script)
#
# KEY DIFFERENCES FROM SERVER TUNING:
#   - Server focuses on inbound connections (somaxconn, SYN backlog, epoll)
#   - Client focuses on outbound connections (ephemeral ports, TIME_WAIT,
#     connection cycling, and large FD counts for many open sockets)
#
# NOTE (Docker): When running load tests inside a container, these settings
# must be applied on the Docker HOST. Containers share the host kernel.
# =============================================================================

# -----------------------------------------------------------------------------
# File Descriptors
# -----------------------------------------------------------------------------
# Each outbound WebSocket connection consumes 1 file descriptor. To sustain
# 100K-1M simultaneous connections we need at least that many FDs plus
# overhead for the load test tool itself (timers, internal pipes, log files).
# 2M provides comfortable headroom.
fs.file-max = 2097152
fs.nr_open = 2097152

# -----------------------------------------------------------------------------
# Ephemeral Port Range
# -----------------------------------------------------------------------------
# The default range (32768-60999) provides only ~28K ports, which is far
# below the 100K+ connections target. Expanding to 1024-65535 gives ~64K
# ports per source IP.
#
# IMPORTANT: For more than ~64K connections from a single machine, you must
# bind to multiple source IP addresses (IP aliases on the NIC). Each source
# IP gets its own full ephemeral port range, so N IPs = N * 64K ports.
# Example:
#   sudo ip addr add 10.0.0.11/24 dev eth0
#   sudo ip addr add 10.0.0.12/24 dev eth0
#   ... then configure the load test tool to round-robin across source IPs.
net.ipv4.ip_local_port_range = 1024 65535

# -----------------------------------------------------------------------------
# TIME_WAIT Handling
# -----------------------------------------------------------------------------
# During load testing, connections are created and torn down rapidly. Without
# these settings, TIME_WAIT sockets accumulate and exhaust the ephemeral port
# range within seconds.

# Allow reuse of TIME_WAIT sockets for new outbound connections when the
# protocol state permits it (timestamps must be enabled, which is the default).
net.ipv4.tcp_tw_reuse = 1

# Reduce FIN_WAIT2 timeout from the default 60s to 10s. During load tests
# we cycle connections aggressively and want resources freed quickly.
# NOTE: This is more aggressive than the server setting (15s) because the
# client side can tolerate faster socket recycling.
net.ipv4.tcp_fin_timeout = 10

# -----------------------------------------------------------------------------
# TCP Buffer Sizes
# -----------------------------------------------------------------------------
# Load test clients send many small WebSocket frames (chat messages, pings).
# Keep default buffers modest (16KB) to minimize per-connection kernel memory,
# allowing more simultaneous connections within available RAM.
#
# Memory math at 100K connections:
#   100K * (16KB recv + 16KB send) = ~3.2 GB kernel buffer memory
# At 500K connections: ~16 GB. Adjust defaults down if RAM is limited.
#
# The max values (16MB) support rare large transfers without constraining
# TCP auto-tuning.
net.ipv4.tcp_rmem = 4096 16384 65536
net.ipv4.tcp_wmem = 4096 16384 65536
net.core.rmem_default = 16384
net.core.wmem_default = 16384
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216

# -----------------------------------------------------------------------------
# TCP Memory (global, in pages of 4KB)
# -----------------------------------------------------------------------------
#   min      = 786432  pages  (~3 GB)  - below this, TCP does not reclaim
#   pressure = 1048576 pages  (~4 GB)  - TCP starts to reduce memory usage
#   max      = 1572864 pages  (~6 GB)  - hard ceiling for all TCP sockets
net.ipv4.tcp_mem = 786432 1048576 1572864

# -----------------------------------------------------------------------------
# Connection Establishment
# -----------------------------------------------------------------------------
# Speed up SYN retransmissions. The default (6 retries) means a failed
# connection attempt takes ~127 seconds to timeout. For load testing we want
# to fail fast so the test framework can report errors quickly.
net.ipv4.tcp_syn_retries = 2

# SYN-ACK retries for any listening sockets on the client (e.g., metrics
# endpoints, debug servers). Keep low to match the fast-fail philosophy.
net.ipv4.tcp_synack_retries = 2

# -----------------------------------------------------------------------------
# TCP Optimization
# -----------------------------------------------------------------------------
# TCP Fast Open for both client (1) and server (2) roles. Saves a round
# trip on repeated connections to the same server, useful when the load
# test ramps up in waves.
net.ipv4.tcp_fastopen = 3

# Listen backlog for any sockets the load test tool opens (metrics, control).
net.core.somaxconn = 65535

# Packet backlog when the NIC delivers faster than the kernel processes.
# High connection counts generate bursts of ACKs and small frames.
net.core.netdev_max_backlog = 65535

# Maximum orphaned TCP sockets (not attached to any process FD). During
# abrupt test teardown, thousands of sockets may become orphans briefly.
net.ipv4.tcp_max_orphans = 262144

# SYN backlog for half-open connections. Generous value absorbs burst
# connection establishment from the load test tool.
net.ipv4.tcp_max_syn_backlog = 65535

# -----------------------------------------------------------------------------
# Connection Tracking (NAT / Firewall)
# -----------------------------------------------------------------------------
# If the load test client is behind a NAT gateway or firewall that uses
# conntrack (nf_conntrack), the default table size (65536) will be exhausted
# at high connection counts, causing packet drops.
#
# Uncomment the following lines ONLY if the nf_conntrack module is loaded:
#   lsmod | grep nf_conntrack
#
# net.netfilter.nf_conntrack_max = 2097152
# net.netfilter.nf_conntrack_tcp_timeout_time_wait = 30
# net.netfilter.nf_conntrack_tcp_timeout_established = 600
